# Ethical Web Scraping Guidelines

Este proyecto implementa un sistema de web scraping Ã©tico diseÃ±ado para ser respetuoso con los servidores web y cumplir con las mejores prÃ¡cticas de la industria.

## ğŸŒŸ CaracterÃ­sticas Ã‰ticas Implementadas

### 1. **Respeto a robots.txt**
- âœ… VerificaciÃ³n automÃ¡tica de robots.txt antes de realizar scraping
- âœ… Parseo bÃ¡sico de directivas User-agent y Disallow
- âœ… Respeto automÃ¡tico a las restricciones encontradas

### 2. **User-Agent Identificativo**
```
EthicalBot/1.0 (Contact: your-email@domain.com)
```
- âœ… User-Agent claro y identificable
- âœ… InformaciÃ³n de contacto incluida
- âœ… VersiÃ³n del bot especificada

### 3. **Rate Limiting Inteligente**
- âœ… LÃ­mite de 10 requests por minuto por IP
- âœ… Delay de 2 segundos entre requests
- âœ… MÃ¡ximo 2 pÃ¡ginas concurrentes por instancia

### 4. **OptimizaciÃ³n de Recursos**
- âœ… Bloqueo de recursos innecesarios (imÃ¡genes, CSS, fuentes)
- âœ… Timeout configurado para evitar conexiones colgadas
- âœ… Uso eficiente de memoria y CPU

### 5. **Respeto al Servidor**
- âœ… Headers HTTP apropiados
- âœ… Manejo de errores HTTP
- âœ… VerificaciÃ³n de estado de respuesta

## ğŸš€ Uso del API

### Health Check
```bash
GET /health
```

### Scraping Endpoint
```bash
POST /scrape
Content-Type: application/json

{
  "url": "https://example.com",
  "selector": "h1"  # Opcional, por defecto "body"
}
```

### Ejemplo con curl
```bash
curl -X POST http://your-service-url/scrape \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "selector": "h1"}'
```

## ğŸ”§ ConfiguraciÃ³n Ã‰tica

### Variables de Entorno
```bash
# Opcional: Personalizar configuraciÃ³n
SCRAPING_DELAY=2000          # Delay entre requests (ms)
MAX_CONCURRENT_PAGES=2       # PÃ¡ginas concurrentes mÃ¡ximas
REQUEST_TIMEOUT=30000        # Timeout de request (ms)
```

### Rate Limiting
El sistema implementa rate limiting basado en IP:
- **LÃ­mite**: 10 requests por minuto
- **Respuesta**: HTTP 429 con tiempo de espera
- **Reinicio**: AutomÃ¡tico cada minuto

## ğŸ›¡ï¸ Medidas de Seguridad

### Container Security
- âœ… Usuario no-root en el contenedor
- âœ… Imagen base Alpine (minimal)
- âœ… Health checks configurados
- âœ… Resource limits en Fargate

### Network Security
- âœ… Security Groups restrictivos
- âœ… VPC privada recomendada
- âœ… Load Balancer como Ãºnico punto de entrada

## ğŸ“‹ Buenas PrÃ¡cticas Recomendadas

### Para Administradores
1. **Monitoreo**: Configurar CloudWatch alarms
2. **Logs**: Revisar logs regularmente para patrones sospechosos
3. **Scaling**: Ajustar auto-scaling segÃºn necesidades
4. **Updates**: Mantener dependencias actualizadas

### Para Usuarios
1. **Frequency**: No hacer requests excesivos
2. **Targets**: Solo hacer scraping de sitios pÃºblicos
3. **Data**: No extraer datos personales o sensibles
4. **Compliance**: Verificar tÃ©rminos de servicio del sitio

## ğŸš¨ Restricciones y Limitaciones

### Lo que NO hace este scraper:
- âŒ No ignora robots.txt
- âŒ No hace requests masivos sin delays
- âŒ No usa User-Agents falsos o engaÃ±osos
- âŒ No intenta evadir rate limits
- âŒ No hace scraping de contenido protegido por login

### Sitios ProblemÃ¡ticos:
- âŒ Sitios con CAPTCHA obligatorio
- âŒ Sitios que requieren JavaScript complejo
- âŒ Sitios con anti-bot avanzado
- âŒ Contenido detrÃ¡s de paywall

## ğŸ“Š Monitoreo y Logs

### Logs Importantes
```bash
# Ver logs en AWS CloudWatch
aws logs tail /ecs/ethical-scraper --follow

# MÃ©tricas importantes:
- Request count per minute
- Error rate
- Response times
- Robots.txt violations (should be 0)
```

### CloudWatch Metrics
- **CPUUtilization**: Debe estar < 70%
- **MemoryUtilization**: Debe estar < 80%
- **RequestCount**: Monitor para patrones anormales
- **TargetResponseTime**: Debe estar < 5s

## ğŸ”„ Mantenimiento

### Updates Regulares
```bash
# Rebuild y redeploy
./deploy.sh

# Verificar salud del servicio
curl http://your-service-url/health
```

### Backup y Recovery
- ConfiguraciÃ³n almacenada en CloudFormation
- ImÃ¡genes versionadas en ECR
- Logs persistidos en CloudWatch

## ğŸ“ Soporte

Si encuentras problemas o necesitas ajustar la configuraciÃ³n Ã©tica, verifica:

1. **Logs de aplicaciÃ³n** en CloudWatch
2. **MÃ©tricas de rate limiting** 
3. **Respuestas HTTP** del sitio objetivo
4. **ConfiguraciÃ³n de robots.txt** del sitio

---

**Recuerda**: El scraping Ã©tico es responsabilidad de todos. Usa esta herramienta de manera responsable y siempre respeta los tÃ©rminos de servicio de los sitios web.